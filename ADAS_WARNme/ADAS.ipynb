{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30e1d96f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b61ff4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in c:\\users\\user\\anaconda3\\lib\\site-packages (0.8.10.1)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mediapipe) (21.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\lib\\site-packages (from mediapipe) (1.22.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\anaconda3\\lib\\site-packages (from mediapipe) (3.4.3)\n",
      "Requirement already satisfied: protobuf<4,>=3.11 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mediapipe) (3.20.1)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\user\\anaconda3\\lib\\site-packages (from mediapipe) (4.6.0.66)\n",
      "Requirement already satisfied: absl-py in c:\\users\\user\\anaconda3\\lib\\site-packages (from mediapipe) (1.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (8.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (3.0.4)\n",
      "Requirement already satisfied: six in c:\\users\\user\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib->mediapipe) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fefef303",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement cv2 (from versions: none)\n",
      "ERROR: No matching distribution found for cv2\n",
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21d86efa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faf47452",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\user\\anaconda3\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: protobuf>=3.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (3.20.1)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (1.22.3)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a0c1da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\user\\anaconda3\\lib\\site-packages (22.2.2)\n",
      "Collecting pip\n",
      "  Downloading pip-22.3.1-py3-none-any.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 2.5 MB/s eta 0:00:00\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.2.2\n",
      "    Uninstalling pip-22.2.2:\n",
      "      Successfully uninstalled pip-22.2.2\n",
      "Successfully installed pip-22.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bf103c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"C:\\Users\\USER\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\n  File \"C:\\Users\\USER\\anaconda3\\lib\\imp.py\", line 296, in find_module\n    raise ImportError(_ERR_MSG.format(name), name=name)\nImportError: No module named '_pywrap_tensorflow'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\USER\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 66, in <module>\n    from tensorflow.python import pywrap_tensorflow\n  File \"C:\\Users\\USER\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\n    _pywrap_tensorflow = swig_import_helper()\n  File \"C:\\Users\\USER\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\n    import _pywrap_tensorflow\nModuleNotFoundError: No module named '_pywrap_tensorflow'\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescription\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_pywrap_tensorflow'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\imp.py\u001b[0m in \u001b[0;36mfind_module\u001b[1;34m(name, path)\u001b[0m\n\u001b[0;32m    295\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ERR_MSG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named '_pywrap_tensorflow'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;31m# use `dlopen()` for dynamic loading.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0m_pywrap_tensorflow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[1;32mimport\u001b[0m \u001b[0m_pywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_pywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named '_pywrap_tensorflow'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13068/1589520110.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[1;33m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[1;32m---> 72\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;31m# Protocol buffers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\Users\\USER\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\n  File \"C:\\Users\\USER\\anaconda3\\lib\\imp.py\", line 296, in find_module\n    raise ImportError(_ERR_MSG.format(name), name=name)\nImportError: No module named '_pywrap_tensorflow'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\USER\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 66, in <module>\n    from tensorflow.python import pywrap_tensorflow\n  File \"C:\\Users\\USER\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\n    _pywrap_tensorflow = swig_import_helper()\n  File \"C:\\Users\\USER\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\n    import _pywrap_tensorflow\nModuleNotFoundError: No module named '_pywrap_tensorflow'\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de60015e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.9.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pygame import mixer\n",
    "from scipy.spatial import distance as dist\n",
    "#from tensorflow import keras\n",
    "\n",
    "#opencv objects detection\n",
    "with open(\"ClassNames.txt\", \"r\") as fd:\n",
    "    classNames = fd.read().splitlines()\n",
    "\n",
    "#Audio Files\n",
    "TiredAudioFile = \"AUDIO/GetSomeRest.wav\"\n",
    "DistratedAudioFile = \"AUDIO/StayFocused.wav\"\n",
    "ClosedEyesAudioFile = \"AUDIO/WakeUp.wav\"\n",
    "SmokingAudioFile = \"AUDIO/StopSmoking.wav\"\n",
    "FoodAudioFile = \"AUDIO/StopEating.wav\"\n",
    "PhoneUsageAudioFile = \"AUDIO/PutDownPhone.wav\"\n",
    "#classeIdsof objects to detecte\n",
    "classes= ['1', 30, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 84]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afd47d2",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bab7a3",
   "metadata": {},
   "source": [
    "### FIRST_CONFIGURATION()\n",
    "To adapte our model to the different camera positions, we have to save the default position of the driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c0aaf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FIRST_CONFIGURATION(draw=False, camera=0, info=True):\n",
    "    face_length = 400\n",
    "    pTime = 0\n",
    "    cap = cv2.VideoCapture(camera)\n",
    "\n",
    "    while cv2.waitKey(1) != ord('d') :\n",
    "        success, img= cap.read()\n",
    "        if not success:\n",
    "            print(\"No camera detected\")\n",
    "            break\n",
    "        #converting the image from BGR to RGB\n",
    "        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        results = faceMesh.process(imgRGB)\n",
    "        faces = []\n",
    "        if results.multi_face_landmarks:\n",
    "            for faceLms in results.multi_face_landmarks:\n",
    "                if draw:\n",
    "                    mpDraw.draw_landmarks(img,faceLms,mpFaceMesh.FACEMESH_CONTOURS,drawSpec,drawSpec)\n",
    "                face = []\n",
    "                for id, lm in enumerate(faceLms.landmark):\n",
    "                    ih, iw, ic = img.shape\n",
    "                    x,y = int(lm.x*iw), int(lm.y*ih)\n",
    "                    #cv2.putText(img, str(id), (x,y),cv2.FONT_HERSHEY_PLAIN,0.8, (0,255,0), 1)\n",
    "                    face.append([x,y])\n",
    "                #calculation distances and facing direction\n",
    "                face_length = dist.euclidean(face[10],face[152])\n",
    "                left_dist_cfg = dist.euclidean(face[1],face[361])\n",
    "                right_dist_cfg = dist.euclidean(face[1],face[132])\n",
    "                direction_cfg= (right_dist_cfg-left_dist_cfg)/face_length\n",
    "                #afficher la direction [-1,1]\n",
    "                if info:\n",
    "                    cv2.putText(img, str(round(direction_cfg,2)), (400,400),cv2.FONT_HERSHEY_PLAIN,2, (0,0,255), 3)\n",
    "                faces.append(face)\n",
    "        # fps calcul \n",
    "        cTime = time.time()\n",
    "        fps = 1/(cTime-pTime)\n",
    "        pTime = cTime \n",
    "        cv2.putText(img, \"Take your default driving position and press 'd'\", (20,70),cv2.FONT_HERSHEY_PLAIN,1.5, (255,100,200), 3)\n",
    "        cv2.putText(img,f'FPS: {int(fps)}',(70,400),cv2.FONT_HERSHEY_PLAIN, 2, (0,255,0), 2)\n",
    "        cv2.imshow(\"First configuration\", img)\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return direction_cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b12964",
   "metadata": {},
   "source": [
    "### Time_thresh()\n",
    "To make the alerts smarter and more accurate we used this function to adapte the behavious time threshold to the speed of the vehicle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75699d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Time_thresh(speed,MinThresh=2,MaxThresh=10,curve=3):\n",
    "    return(((MaxThresh-MinThresh)/(np.exp(speed/curve)))+MinThresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee91562",
   "metadata": {},
   "source": [
    "Simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6372403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x=np.array(range(101))\n",
    "Speed = 0\n",
    "acceleration = True\n",
    "while cv2.waitKey(10) != 27:\n",
    "    plt.xlabel(\"Speed (km/h)\")\n",
    "    plt.ylabel(\"Time threshold (s)\")\n",
    "    plt.grid('on')\n",
    "    plt.plot(x,Time_thresh(x,curve=5),'r')\n",
    "    plt.plot(Speed, Time_thresh(Speed,curve=5), 'ko')\n",
    "    plt.savefig('ploy.jpg')\n",
    "    img = cv2.imread('ploy.jpg')\n",
    "    cv2.putText(img, str(Speed)+'km/h', (200,60),cv2.FONT_HERSHEY_PLAIN,1.5, (0,0,255), 2)\n",
    "    cv2.putText(img, 'Press ESC to quit', (10,20),cv2.FONT_HERSHEY_PLAIN,1, (0,0,0), 1)\n",
    "    cv2.imshow(\"Time Threshold depending on the speed \",img)\n",
    "    if acceleration:\n",
    "        Speed += 5\n",
    "    else:\n",
    "        Speed -= 5\n",
    "    if Speed==100:\n",
    "        acceleration = False\n",
    "    if Speed==0:\n",
    "        acceleration = True\n",
    "    plt.clf()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4302d98",
   "metadata": {},
   "source": [
    "### Calculations()\n",
    "Calculates the differente distances of the crucial point to analyse face muscles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a740f056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calculations(face):\n",
    "    left_dist = dist.euclidean(face[1],face[361])\n",
    "    right_dist = dist.euclidean(face[1],face[132])\n",
    "    down_dist = dist.euclidean(face[1],face[0])\n",
    "    \n",
    "    upper_lip=[((face[13][0]+face[312][0]+face[267][0]+face[0][0]+face[37][0]+face[82][0])/6),\n",
    "               ((face[13][1]+face[312][1]+face[267][1]+face[0][1]+face[37][1]+face[82][1])/6)]\n",
    "    lower_lip=[((face[14][0]+face[317][0]+face[314][0]+face[17][0]+face[84][0]+face[87][0])/6),\n",
    "               ((face[14][1]+face[317][1]+face[314][1]+face[17][1]+face[84][1]+face[87][1])/6)]\n",
    "    \n",
    "    A_eye_R = dist.euclidean(face[160],face[144])\n",
    "    B_eye_R = dist.euclidean(face[158],face[153])\n",
    "    C_eye_R = dist.euclidean(face[33],face[133])\n",
    "    R_ear = (A_eye_R + B_eye_R) / (2.0 * C_eye_R)\n",
    "\n",
    "    A_eye_L = dist.euclidean(face[385],face[380])\n",
    "    B_eye_L = dist.euclidean(face[387],face[373])\n",
    "    C_eye_L = dist.euclidean(face[362],face[263])\n",
    "    L_ear = (A_eye_L + B_eye_L) / (2.0 * C_eye_L)\n",
    "    \n",
    "    ear = (R_ear + L_ear) / 2.0\n",
    "    dist1 = dist.euclidean(upper_lip,lower_lip)\n",
    "    \n",
    "    return dist1,left_dist,right_dist,down_dist,ear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14c5d9b",
   "metadata": {},
   "source": [
    "### Model()\n",
    "The main code of ADAS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24ca311d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model(draw=True,camera=0, OBJ_DETECTION=True, DISTRACTION=True, DROWSINESS=True, SMOKING=False,\n",
    "          audio_alarm=True, EYE_AR_THRESH=0.2, TIME_closed_eyes=3,TIME_open_mouth=2, Nb_Yawn=3,\n",
    "          Alarm_time_yawn=60, Time_Alarm_Loop=5, direction_cfg = 0):\n",
    "    \n",
    "    #Initialisations\n",
    "    face_length = 400\n",
    "    down_threshold = 30\n",
    "    EYE_AR_CONSEC_FRAMES = 40\n",
    "    YAWN_THRESH = 30\n",
    "    YAWN_CONSEC_FRAMES = 40\n",
    "    DIST_CONSEC_FRAMES = 30\n",
    "    ALARM_Thresh = 30\n",
    "    cpt_yawn_per_minute = 0 \n",
    "    alarm_status = False\n",
    "    alarm_status2 = False\n",
    "    cpt = cpt2 = cpt3 = cpt4 = cpt5 = cpt6 = cpt7 = 0\n",
    "    speed = 80 # km/h\n",
    "\n",
    "    #OBJ DETECTION confguration\n",
    "    configPath = 'ssd_mobilenet_v3_large_coco_2020_01_14.pbtxt'\n",
    "    weightsPath = 'frozen_inference_graph.pb'\n",
    "    net = cv2.dnn_DetectionModel(weightsPath,configPath)\n",
    "    net.setInputSize(300,300)\n",
    "    net.setInputScale(1.0/127.5)\n",
    "    net.setInputMean((127.5,127.5,127.5))\n",
    "    net.setInputSwapRB(True)\n",
    "\n",
    "    cap = cv2.VideoCapture(camera)\n",
    "    pTime = 0\n",
    "    while cv2.waitKey(1) != 27 :\n",
    "        #vehicle speed simulation:\n",
    "    #     if cv2.waitKey(1) == ord('a'):\n",
    "    #         speed += 10\n",
    "    #     if cv2.waitKey(1) == ord('d'):\n",
    "    #         speed -= 10\n",
    "    #     if speed> 100:\n",
    "    #         speed = 0\n",
    "    #     if speed<0:\n",
    "    #         speed = 100\n",
    "        #cv2.putText(display,str(round(speed))+'km/h',(200,100),cv2.FONT_HERSHEY_COMPLEX,.8,(0,255,0),2)\n",
    "        success, img= cap.read()\n",
    "        if not success:\n",
    "            print(\"No image!\")\n",
    "            break\n",
    "        display = img\n",
    "        faces = []\n",
    "\n",
    "        ########## OpenCV Object Detection ##########\n",
    "        if OBJ_DETECTION:\n",
    "            classIds, confs, bbox = net.detect(img,confThreshold=0.5)\n",
    "            if len(classIds) != 0:\n",
    "                for classId,confidence,box in zip(classIds.flatten(),confs.flatten(),bbox):\n",
    "                    if classId in classes:\n",
    "                        if draw:\n",
    "                            cv2.rectangle(display,box,color=(0,255,0),thickness=2)\n",
    "                            cv2.putText(display,classNames[classId].upper(),(box[0]+10,box[1]+30),cv2.FONT_HERSHEY_COMPLEX,.8,(0,255,0),2)\n",
    "                        if audio_alarm:\n",
    "                            cpt3 += 1\n",
    "                            if cpt3 == 1:\n",
    "                                mixer.Sound(FoodAudioFile).play()\n",
    "                            if cpt3 >= ALARM_Thresh:\n",
    "                                cpt3 = 0\n",
    "                        cv2.putText(display,'FOOD DETECTED',(200,400),cv2.FONT_HERSHEY_COMPLEX,.8,(0,0,255),2)\n",
    "\n",
    "                    if classId==77:\n",
    "                        \n",
    "                        cv2.rectangle(display,box,color=(0,255,0),thickness=2)\n",
    "                        cv2.putText(display,'CELLPHONE DETECTED',(200,400),cv2.FONT_HERSHEY_COMPLEX,.8,(0,0,255),2)\n",
    "                        if audio_alarm:\n",
    "                            cpt4 += 1\n",
    "                            if cpt4 == 1: \n",
    "                                mixer.Sound(PhoneUsageAudioFile).play()\n",
    "                            if cpt4 >= ALARM_Thresh:\n",
    "                                cpt4 = 0\n",
    "\n",
    "        ########## Smoking Detection ##########\n",
    "        if SMOKING:\n",
    "            predicted=[]\n",
    "            predicted.append(cv2.resize(img, (224,224)))\n",
    "            pred=model.predict(np.array(predicted)/255)\n",
    "            cv2.putText(display,str(round(pred[0][0],2)),(200,200),cv2.FONT_HERSHEY_COMPLEX,.8,(0,0,255),2)\n",
    "            if pred[0][0]>0.5:\n",
    "                cv2.putText(display,'Smoking DETECTED',(200,100),cv2.FONT_HERSHEY_COMPLEX,.8,(0,255,0),2)\n",
    "\n",
    "                if audio_alarm:\n",
    "                    cpt6 +=1\n",
    "                    if cpt6 == 4:\n",
    "                        mixer.Sound(SmokingAudioFile).play()\n",
    "                    if cpt6 >= ALARM_Thresh:\n",
    "                        cpt6 = 0\n",
    "\n",
    "        ########## Mediapipe FaceMesh Detection ##########\n",
    "        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        if DISTRACTION or DROWSINESS:\n",
    "            results = faceMesh.process(imgRGB)\n",
    "            if results.multi_face_landmarks:\n",
    "                for faceLms in results.multi_face_landmarks:\n",
    "                    if draw:\n",
    "                        mpDraw.draw_landmarks(display,faceLms,mpFaceMesh.FACEMESH_CONTOURS,drawSpec,drawSpec)\n",
    "                    face = []\n",
    "                    for id, lm in enumerate(faceLms.landmark):\n",
    "                        ih, iw, ic = img.shape\n",
    "                        x,y,z = int(lm.x*iw), int(lm.y*ih), int(lm.z*ic)\n",
    "                        # to show the landmarks ids\n",
    "                        #cv2.putText(img, str(id), (x,y),cv2.FONT_HERSHEY_PLAIN,0.8, (0,255,0), 1)\n",
    "                        face.append([x,y,z])\n",
    "\n",
    "                    down_threshold = down_threshold / face_length\n",
    "                    face_length = dist.euclidean(face[10],face[152])\n",
    "                    down_threshold = down_threshold * face_length\n",
    "                    \n",
    "                    dist1,left_dist,right_dist,down_dist,ear= Calculations(face)\n",
    "                    cv2.putText(img, str(round(DIST_CONSEC_FRAMES)), (150,160),cv2.FONT_HERSHEY_PLAIN,2, (0,0,255), 2)\n",
    "                    direction= (right_dist-left_dist)/face_length\n",
    "\n",
    "\n",
    "                    ########## Alarm conditions ##########\n",
    "                    if DISTRACTION and speed!=0:\n",
    "                        if direction<-0.5+direction_cfg or down_dist<=down_threshold or direction>0.5+direction_cfg:\n",
    "                            cpt7 += 1\n",
    "                            #cv2.putText(display, str(cpt7)+'/'+str(DIST_CONSEC_FRAMES), (150,250),cv2.FONT_HERSHEY_PLAIN,2, (0,0,255),3)\n",
    "                            if cpt7 >= DIST_CONSEC_FRAMES:\n",
    "                                mixer.Sound(DistratedAudioFile).play()\n",
    "                                cpt7 = 0\n",
    "                        else:\n",
    "                            cpt7 = 0\n",
    "\n",
    "                    if DROWSINESS and speed!=0:\n",
    "                        if ear < EYE_AR_THRESH:\n",
    "                            cpt += 1\n",
    "                            cv2.putText(display, str(cpt), \n",
    "                                    (50,100),cv2.FONT_HERSHEY_PLAIN,\n",
    "                                    3, (0,0,255),2)\n",
    "                            if cpt >= EYE_AR_CONSEC_FRAMES:\n",
    "                                cv2.putText(display, 'SLEEP ALERT!', (120,100),cv2.FONT_HERSHEY_PLAIN,3, (0,0,255),2)\n",
    "                                cpt = 0 #\n",
    "                                if alarm_status == False:\n",
    "                                    alarm_status = True\n",
    "                                    if audio_alarm:\n",
    "                                        mixer.Sound(ClosedEyesAudioFile).play()\n",
    "                                        alarm_status = False #\n",
    "\n",
    "                        else:\n",
    "                            cpt = 0\n",
    "                            alarm_status = False\n",
    "\n",
    "                        YAWN_THRESH = dist.euclidean(face[78],face[308])/1.5\n",
    "\n",
    "                        if dist1 > YAWN_THRESH:\n",
    "                            cpt2 += 1\n",
    "                            if cpt2 == 1:\n",
    "                                yawn = True\n",
    "                            cv2.putText(display, str(cpt2), (50,150),cv2.FONT_HERSHEY_PLAIN, 3, (0,0,255),2)\n",
    "                            if cpt2 >= YAWN_CONSEC_FRAMES:\n",
    "                                cv2.putText(img, 'Yawn alert', (120,150),cv2.FONT_HERSHEY_PLAIN,3, (0,0,255),2)\n",
    "                                if yawn == True:\n",
    "                                    yawn = False\n",
    "                                    cpt_yawn_per_minute += 1\n",
    "                                    if cpt_yawn_per_minute == 1:\n",
    "                                        tm = time.time()\n",
    "                                if cpt_yawn_per_minute >=  Nb_Yawn:\n",
    "                                    if time.time() - tm <= Alarm_time_yawn :\n",
    "                                        if alarm_status2 == False:\n",
    "                                            alarm_status2 = True\n",
    "                                            if audio_alarm:\n",
    "                                                mixer.Sound(TiredAudioFile).play()\n",
    "                                    else:\n",
    "                                        tm = time.time()\n",
    "                                        cpt_yawn_per_minute = 1\n",
    "                        else:\n",
    "                            yawn = False\n",
    "                            cpt2 = 0\n",
    "                            alarm_status2 = False\n",
    "\n",
    "\n",
    "\n",
    "                        cv2.putText(display, 'Yawn: '+str(round(dist1,2)), \n",
    "                                    (10,200),cv2.FONT_HERSHEY_PLAIN,\n",
    "                                    1.5, (0,255,0),1)\n",
    "                        cv2.putText(display, 'EAR: '+str(round(ear,2)), \n",
    "                                    (10,240),cv2.FONT_HERSHEY_PLAIN,\n",
    "                                    1.5, (0,255,0),1)\n",
    "\n",
    "                        #\n",
    "                        cv2.putText(display, 'Yawn per minute : '+str(cpt_yawn_per_minute), \n",
    "                                    (10,280),cv2.FONT_HERSHEY_PLAIN,\n",
    "                                    1.5, (0,255,0),1)\n",
    "                        \n",
    "                    faces.append(face)\n",
    "    # Hot-keys short-cut\n",
    "    #     if Help:\n",
    "    #         cv2.putText(img,\"'D': Detect drowsiness \",(20,340),cv2.FONT_HERSHEY_PLAIN, 1, (10,50,10), 2)\n",
    "    #         cv2.putText(img,\"'S': Detect distraction \",(20,380),cv2.FONT_HERSHEY_PLAIN, 1, (10,50,10), 2)\n",
    "    #         cv2.putText(img,\"'O': Detect objects \",(20,420),cv2.FONT_HERSHEY_PLAIN, 1, (10,50,10), 2)\n",
    "    #         cv2.putText(img,\"'H': Show this information \",(20,450),cv2.FONT_HERSHEY_PLAIN, 1, (10,50,10), 2)\n",
    "\n",
    "    #     if cv2.waitKey(1) ==  ord('h'):\n",
    "    #         if Help:\n",
    "    #             Help = False\n",
    "    #         else:\n",
    "    #             Help = True\n",
    "    #     if cv2.waitKey(1) ==  ord('d'):\n",
    "    #         if DROWSINESS:\n",
    "    #             DROWSINESS = False\n",
    "    #         else:\n",
    "    #             DROWSINESS = True\n",
    "    #     if cv2.waitKey(1) ==  ord('s'):\n",
    "    #         if DISTRACTION:\n",
    "    #             DISTRACTION = False\n",
    "    #         else:\n",
    "    #             DISTRACTION = True\n",
    "    #     if cv2.waitKey(1) ==  ord('o'):\n",
    "    #         if OBJ_DETECTION:\n",
    "    #             OBJ_DETECTION = False\n",
    "    #         else:\n",
    "    #             OBJ_DETECTION = True\n",
    "\n",
    "        #calcul of fps\n",
    "        cTime = time.time()\n",
    "        fps = 1/(cTime-pTime)\n",
    "        #readapt thresholds according to your fps\n",
    "        DIST_CONSEC_FRAMES = fps * Time_thresh(speed,2,20,5)\n",
    "        EYE_AR_CONSEC_FRAMES = fps * Time_thresh(speed,2,8,3)\n",
    "        YAWN_CONSEC_FRAMES = fps * TIME_open_mouth\n",
    "        ALARM_Thresh = fps * Time_Alarm_Loop\n",
    "        pTime = cTime \n",
    "        cv2.putText(img,f'FPS: {int(fps)}',(20,70),cv2.FONT_HERSHEY_PLAIN, 3, (0,255,0), 3)\n",
    "        cv2.imshow(\"ADAS\", display)\n",
    "    # to show vehicle velocity:\n",
    "    #     if cv2.waitKey(1) == ord('a'):\n",
    "    #         cv2.imwrite(\"image4.png\",display)\n",
    "\n",
    "    #     plt.xlabel(\"Speed (km/h)\")\n",
    "    #     plt.ylabel(\"Time threshold (s)\")\n",
    "    #     plt.grid('on')\n",
    "    #     x=np.array(range(50))\n",
    "    #     plt.plot(x,Time_thresh(x),'r')\n",
    "    #     plt.plot(speed, Time_thresh(speed), 'bo')\n",
    "    #     plt.savefig('ploy.jpg')\n",
    "    #     cv2.imshow(\"plot\",cv2.imread('ploy.jpg'))\n",
    "    #     plt.clf()\n",
    "\n",
    "    #release the camera and close the windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b916bb90",
   "metadata": {},
   "source": [
    "# Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3611251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw = True\n",
    "First_cfg = False\n",
    "\n",
    "OBJ_DETECTION = True # unable/disable Object detection\n",
    "DISTRACTION = True # unable/disable distraction detection\n",
    "DROWSINESS = True # unable/disable drowsiness detection\n",
    "SMOKING = False # unable/disable smoking detection\n",
    "\n",
    "audio_alarm = True # unable/disable audio alarms\n",
    "EYE_AR_THRESH = 0.2 # Eye aspect ratio threshold\n",
    "TIME_open_mouth = 2 # seconds of open mouth frames to detect a yawn\n",
    "TIME_closed_eyes = 2 # seconds of closed eyes to detect sleeping\n",
    "Nb_Yawn = 3 # Number of Yawnings per 'Alarm_time_yawn'\n",
    "Alarm_time_yawn = 60 #Seconds in which yawn counter will be reset\n",
    "Time_Alarm_Loop = 3   #Seconds \n",
    "camera = 1 #Camera id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f9b6b7",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08dc0afc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15196/4005913113.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmixer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'smoking_model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#face mesh params and drawing options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmpDraw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msolutions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrawing_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmpFaceMesh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msolutions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mface_mesh\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "mixer.init()\n",
    "model = keras.models.load_model('smoking_model')\n",
    "#face mesh params and drawing options\n",
    "mpDraw = mp.solutions.drawing_utils\n",
    "mpFaceMesh = mp.solutions.face_mesh\n",
    "faceMesh = mpFaceMesh.FaceMesh(max_num_faces=1)\n",
    "drawSpec = mpDraw.DrawingSpec(thickness=1,circle_radius=0)\n",
    "\n",
    "if First_cfg:\n",
    "    direction_cfg = FIRST_CONFIGURATION()\n",
    "else: \n",
    "    direction_cfg = 0\n",
    "Model(draw,camera, OBJ_DETECTION, DISTRACTION, DROWSINESS,SMOKING,audio_alarm, EYE_AR_THRESH,\n",
    "      TIME_closed_eyes,TIME_open_mouth, Nb_Yawn,Alarm_time_yawn, Time_Alarm_Loop, direction_cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9051b90d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
